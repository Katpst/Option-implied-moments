# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YNYWkBqOqvuxoeLXzhNridQyvcGt2zIH
"""

from pathlib import Path
import pandas as pd
import numpy as np
from scipy.interpolate import UnivariateSpline

# =========================
# PATHS
# =========================
#BASE_DIR = Path(__file__).resolve().parent
#DATA_DIR = BASE_DIR / "data"
#OUTPUT_DIR = BASE_DIR / "outputs"
#OUTPUT_DIR.mkdir(exist_ok=True)

# =========================
# LOAD DATA
# =========================
def load_data():
    caps = pd.read_csv("cleaned_caps_quotes_1y.csv")
    floors = pd.read_csv("cleaned_floors_quotes_1y.csv")
    swaps = pd.read_csv("cleaned_swaps_curves_1y.csv")
    coverage = pd.read_csv("manifest_coverage_1y.csv")

    for df in [caps, floors, swaps, coverage]:
        df["date"] = pd.to_datetime(df["date"])

    return caps, floors, swaps, coverage

# =========================
# MOMENTS
# =========================
def compute_moments(x, pdf):
    x = np.asarray(x)
    pdf = np.asarray(pdf)

    if x.ndim != 1 or pdf.ndim != 1 or len(x) != len(pdf):
        raise ValueError("x and pdf must be 1D arrays of the same length.")
    if len(x) < 3:
        raise ValueError("x grid is too short.")

    dx = float(x[1] - x[0])

    pdf = np.clip(pdf, 0, None)
    total_mass = pdf.sum() * dx
    if total_mass <= 0:
        return np.nan, np.nan, np.nan, np.nan
    pdf = pdf / total_mass

    mean = np.sum(x * pdf) * dx
    var = np.sum((x - mean) ** 2 * pdf) * dx
    std = np.sqrt(var) if var > 0 else np.nan

    if not np.isfinite(std) or std <= 0:
        skew = np.nan
        kurt = np.nan
    else:
        skew = np.sum(((x - mean) / std) ** 3 * pdf) * dx
        kurt = np.sum(((x - mean) / std) ** 4 * pdf) * dx

    return mean, var, skew, kurt


def compute_logI_variance(x, pdf):
    x = np.asarray(x)
    pdf = np.asarray(pdf)
    dx = float(x[1] - x[0])

    pdf = np.clip(pdf, 0, None)
    mass = pdf.sum() * dx
    if mass <= 0:
        return np.nan

    pdf = pdf / mass

    logI = np.log(np.clip(1.0 + x, 1e-12, None))
    mean_logI = np.sum(logI * pdf) * dx
    var_logI  = np.sum((logI - mean_logI)**2 * pdf) * dx

    return var_logI



# =========================
# METHOD 1: NON-PARAMETRIC (SPLINE + BL)
# =========================
def build_call_prices_from_caps_floors(one_slice, B, F):
    df = one_slice.copy()
    if "K" not in df.columns:
        df["K"] = 1.0 + df["k"]

    caps = df[df["source"] == "cap"][["K", "price_per_1"]].rename(columns={"price_per_1": "C_obs"})
    floors = df[df["source"] == "floor"][["K", "price_per_1"]].rename(columns={"price_per_1": "P_obs"})

    caps = caps.groupby("K", as_index=False)["C_obs"].mean()
    floors = floors.groupby("K", as_index=False)["P_obs"].mean()

    merged = caps.merge(floors, on="K", how="outer").sort_values("K")

    # Put-call parity to turn puts into calls
    merged["C_from_put"] = merged["P_obs"] + B * (F - merged["K"])

    merged["C"] = merged["C_obs"]
    merged.loc[merged["C"].isna(), "C"] = merged.loc[merged["C"].isna(), "C_from_put"]

    both = merged["C_obs"].notna() & merged["P_obs"].notna()
    merged.loc[both, "C"] = 0.5 * (merged.loc[both, "C_obs"] + merged.loc[both, "C_from_put"])

    merged = merged.dropna(subset=["C"]).sort_values("K")
    return merged[["K", "C"]]

def method1_nonparametric_pdf(one_slice, B, F, smoothing=1e-6, grid_size=800):
    call_df = build_call_prices_from_caps_floors(one_slice, B=B, F=F)
    if len(call_df) < 4:
        return None, None

    K = call_df["K"].to_numpy()
    C = call_df["C"].to_numpy()

    spl = UnivariateSpline(K, C, s=0)  # interpolation exacte

    K_grid = np.linspace(K.min(), K.max(), grid_size)
    d2C = spl.derivative(n=2)(K_grid)

    pdf_K = np.clip(d2C / B, 0, None)

    x = K_grid - 1.0
    dx = x[1] - x[0]
    mass = pdf_K.sum() * dx
    if mass <= 0:
        return None, None

    pdf = pdf_K / mass
    return x, pdf


# =========================
# METHOD 2: PARAMETRIC (Gaussian log I, Fleckenstein-style)
# =========================

def lognormal_shifted_moments(F, var_logI):
    """
    Parametric benchmark: assume ln(I) ~ N(mu, s^2),
    enforce E[I] = F and use var_logI = Var(ln I).
    Output moments of x = I - 1 (inflation rate over horizon).
    """
    s2 = float(var_logI)
    if not np.isfinite(s2) or s2 <= 0 or not np.isfinite(F) or F <= 0:
        return np.nan, np.nan, np.nan, np.nan

    mu = np.log(F) - 0.5 * s2

    # Raw moments of I
    m1 = np.exp(mu + 0.5 * s2)          # = F
    m2 = np.exp(2*mu + 2*s2)
    m3 = np.exp(3*mu + 4.5*s2)
    m4 = np.exp(4*mu + 8*s2)

    # Moments of x = I - 1
    mean = m1 - 1.0
    var  = m2 - m1**2

    if not np.isfinite(var) or var <= 0:
        return mean, np.nan, np.nan, np.nan

    # Central moments (same for I and x, shift doesn't change central moments)
    mu3 = m3 - 3*m2*m1 + 2*m1**3
    mu4 = m4 - 4*m3*m1 + 6*m2*m1**2 - 3*m1**4

    skew = mu3 / (var ** 1.5)
    kurt = mu4 / (var ** 2)

    return mean, var, skew, kurt

# =========================
# MAIN
# =========================
def main():
    caps, floors, swaps, coverage = load_data()

    print("CAPS:", caps.shape)
    print("FLOORS:", floors.shape)
    print("SWAPS:", swaps.shape)
    print("COVERAGE:", coverage.shape)

    options = pd.concat([caps, floors], ignore_index=True)
    options["date"] = pd.to_datetime(options["date"])  # safety

    # debug preview
    out1 = "step1_preview_options.csv"
    options.head(200).to_csv(out1, index=False)
    print("\n Preview saved to:", out1)

    # coverage width
    if "K_min_obs" in coverage.columns and "K_max_obs" in coverage.columns:
        coverage["coverage_width"] = coverage["K_max_obs"] - coverage["K_min_obs"]
    else:
        coverage["coverage_width"] = np.nan

    swaps_key = swaps.set_index(["area", "date"])

    results = []

    for area in sorted(options["area"].unique()):
        dates = sorted(options.loc[options["area"] == area, "date"].unique())

        for d in dates:
            one = options[(options["area"] == area) & (options["date"] == d)].copy()
            n_quotes = len(one)

            cov_row = coverage[(coverage["area"] == area) & (coverage["date"] == d)]
            cov_width = float(cov_row["coverage_width"].iloc[0]) if len(cov_row) == 1 else np.nan

            if (area, d) not in swaps_key.index:
                results.append({
                    "date": d, "area": area, "method": "method1_nonparam_spline",
                    "mean": np.nan, "variance": np.nan, "skewness": np.nan, "kurtosis": np.nan,
                    "n_quotes": n_quotes, "coverage_width": cov_width, "status": "missing_swaps"
                })
                continue

            # robust extraction (handles accidental duplicates)
            row = swaps_key.loc[(area, d)]
            if isinstance(row, pd.DataFrame):
                row = row.iloc[0]

            B = float(row["B"])
            F = float(row["K_star"])

            x, pdf = method1_nonparametric_pdf(one, B=B, F=F, smoothing=1e-3)

            if x is None:
                results.append({
                    "date": d, "area": area, "method": "method1_nonparam_spline",
                    "mean": np.nan, "variance": np.nan, "skewness": np.nan, "kurtosis": np.nan,
                    "n_quotes": n_quotes, "coverage_width": cov_width, "status": "method1_failed"
                })
                continue

            mean, var, skew, kurt = compute_moments(x, pdf)
            var_logI = compute_logI_variance(x, pdf)


            results.append({
             "date": d,
             "area": area,
             "method": "method1_nonparam_spline",
             "mean": mean,
             "variance": var,
             "skewness": skew,
             "kurtosis": kurt,
             "var_logI": var_logI,
             "n_quotes": n_quotes,
             "coverage_width": cov_width,
             "status": "ok" })

            # ---- Method 3: Parametric benchmark (Gaussian ln I) ----
            m_p, v_p, s_p, k_p = lognormal_shifted_moments(F=F, var_logI=var_logI)

            results.append({
            "date": d,
            "area": area,
            "method": "method2_parametric_gaussian_logI",
            "mean": m_p,
            "variance": v_p,
            "skewness": s_p,
            "kurtosis": k_p,
            "var_logI": var_logI,
            "n_quotes": n_quotes,
            "coverage_width": cov_width,
            "status": "ok" if np.isfinite(v_p) else "method2_failed"
        })



    res = pd.DataFrame(results).sort_values(["area", "date", "method"])

    print(res["method"].value_counts())

    out_final = "familynames_option_prices.csv"
    res.to_csv(out_final, index=False)
    print("\n Final saved to:", out_final)

    print("\nStatus counts:")
    print(res["status"].value_counts(dropna=False))



if __name__ == "__main__":
    main()

import matplotlib.pyplot as plt

res = pd.read_csv("familynames_option_prices.csv")
res["date"] = pd.to_datetime(res["date"], dayfirst=True, errors="coerce")

def plot_moment(res, area, moment_col):
    df = res[(res["area"] == area) & (res["status"] == "ok")].copy()
    pivot = df.pivot(index="date", columns="method", values=moment_col).sort_index()

    plt.figure()
    plt.plot(pivot.index, pivot)  # matplotlib will plot each column
    plt.title(f"{area} â€“ {moment_col}")
    plt.xlabel("date")
    plt.ylabel(moment_col)
    plt.legend(pivot.columns, loc="best")
    plt.tight_layout()
    plt.show()

for m in ["mean", "variance", "skewness", "kurtosis"]:
    plot_moment(res, area="EU", moment_col=m)

def disagreement_flags(res, area="EU",
                       methodA="method1_nonparam_spline",
                       methodB="method3_parametric_gaussian_logI",
                       eps_mu=0.002, rel_var=0.10):
    df = res[(res.area==area) & (res.status=="ok")].copy()
    wide = df.pivot(index="date", columns="method", values=["mean","variance","coverage_width","n_quotes","var_logI"])
    wide = wide.sort_index()

    muA = wide["mean"][methodA]
    muB = wide["mean"][methodB]
    vA  = wide["variance"][methodA]
    vB  = wide["variance"][methodB]

    flag_mu  = (muA - muB).abs() > eps_mu
    flag_var = ((vA - vB).abs() / vA.abs()) > rel_var

    out = pd.DataFrame({
        "mean_A": muA, "mean_B": muB,
        "var_A": vA, "var_B": vB,
        "flag_mean": flag_mu,
        "flag_var": flag_var,
        "coverage_width": wide["coverage_width"][methodA],
        "n_quotes": wide["n_quotes"][methodA],
        "var_logI": wide["var_logI"][methodA],
    })
    return out

flags = disagreement_flags(res, area="EU")
print(flags[flags["flag_mean"] | flags["flag_var"]].head(20))

plt.figure()
plt.scatter(flags["coverage_width"], (flags["mean_A"]-flags["mean_B"]).abs())
plt.xlabel("coverage_width")
plt.ylabel("|mean difference|")
plt.title("Mean disagreement vs strike coverage")
plt.tight_layout()
plt.show()

df_var = res[(res.area=="EU") & (res.status=="ok")].pivot(index="date", columns="method", values="variance").sort_index()

plt.figure()
plt.plot(df_var.index, df_var["method1_nonparam_spline"], label="BL variance")
plt.plot(df_var.index, df_var["method3_parametric_gaussian_logI"], label="Parametric variance")

bad = flags[flags["flag_var"]].index
plt.scatter(bad, df_var.loc[bad, "method1_nonparam_spline"], marker="x", label="disagree dates")
plt.legend()
plt.title("Variance time series + disagreement dates")
plt.tight_layout()
plt.show()